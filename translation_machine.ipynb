{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Translation** with **Transformer Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Vietnamese Text with underthesea Vietnamese Natural Language Processing Toolkit\n",
    "\n",
    "## Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torchtext; \n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as pl\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from typing import Iterable, List, Callable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from underthesea import sent_tokenize, text_normalize, word_tokenize\n",
    "from torchmetrics.text import BLEUScore\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from transformer_from_scratch import Transformer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(en_file, vi_file):\n",
    "    \"\"\"\n",
    "    Read text pairs from files, then build a dataframe with two columns: english and vietnamese\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(en_file, 'r') as f:\n",
    "        en_lines = f.readlines()\n",
    "        \n",
    "    with open(vi_file, 'r') as f:\n",
    "        vi_lines = f.readlines()\n",
    "        \n",
    "    data = pd.DataFrame({'english': en_lines, 'vietnamese': vi_lines})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file\n",
    "df = read_text('Data/en_sents', 'Data/vi_sents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>vietnamese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please put the dustpan in the broom closet\\n</td>\n",
       "      <td>xin vui lòng đặt người quét rác trong tủ chổi\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Be quiet for a moment.\\n</td>\n",
       "      <td>im lặng một lát\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Read this\\n</td>\n",
       "      <td>đọc này\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom persuaded the store manager to give him ba...</td>\n",
       "      <td>tom thuyết phục người quản lý cửa hàng trả lại...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friendship consists of mutual understanding\\n</td>\n",
       "      <td>tình bạn bao gồm sự hiểu biết lẫn nhau\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0       Please put the dustpan in the broom closet\\n   \n",
       "1                           Be quiet for a moment.\\n   \n",
       "2                                        Read this\\n   \n",
       "3  Tom persuaded the store manager to give him ba...   \n",
       "4      Friendship consists of mutual understanding\\n   \n",
       "\n",
       "                                          vietnamese  \n",
       "0    xin vui lòng đặt người quét rác trong tủ chổi\\n  \n",
       "1                                  im lặng một lát\\n  \n",
       "2                                          đọc này\\n  \n",
       "3  tom thuyết phục người quản lý cửa hàng trả lại...  \n",
       "4           tình bạn bao gồm sự hiểu biết lẫn nhau\\n  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have total 254090 pairs of sentences.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have total {len(df)} pairs of sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this step, we will preprocess our data to make it suitable for our Transformer model.\n",
    "\n",
    "For English, we will use the spaCy library, which is a powerful tool for natural language processing.\n",
    "\n",
    "For Vietnamese, we will use the underthesea library, which is a Vietnamese Natural Language Processing Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Vietnamese Text with Underthesea\n",
    "\n",
    "### Overview\n",
    "\n",
    "Underthesea is a powerful toolkit for processing Vietnamese language data. It provides functionalities for various tasks such as tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use Underthesea for tokenizing Vietnamese text, follow these steps:\n",
    "\n",
    "1. **Installation**: First, ensure that you have Underthesea installed. If not, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install underthesea\n",
    "```\n",
    "\n",
    "2. **Tokenization**: With Underthesea installed, you can tokenize Vietnamese text by simply calling the `word_tokenize` function on the text. Here is an example:\n",
    "\n",
    "```python\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "text = \"Underthesea là thư viện xử lý ngôn ngữ tự nhiên Tiếng Việt.\"\n",
    "tokens = word_tokenize(text)\n",
    "```\n",
    "\n",
    "In this example, `tokens` will be a list of tokens extracted from the input text.\n",
    "\n",
    "Remember to always refer to the official documentation or repository for the most accurate and updated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models if necessary\n",
    "if not spacy.util.is_package('en_core_web_md'):\n",
    "    spacy.cli.download('en_core_web_md')\n",
    "    \n",
    "# Load the models\n",
    "nlp_en = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vi(text: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a Vietnamese text into sentences and words\n",
    "    \n",
    "    Args:\n",
    "        text (str): the input text\n",
    "        \n",
    "    Returns:\n",
    "        List[List[str]]: a list of sentences, each sentence is a list of tokens\n",
    "    \"\"\"\n",
    "    # Step 1: Sentence Tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Step 2: Text Normalization (assuming it's just lowercasing here)\n",
    "    sentences = [text_normalize(sentence) for sentence in sentences]\n",
    "        \n",
    "    # Step 3: Word Tokenization\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "        \n",
    "    # Flatten the list\n",
    "    tokenized_sentences = [word for sentence in tokenized_sentences for word in sentence]\n",
    "        \n",
    "    # Lowercase all tokens\n",
    "    tokenized_sentences = [word.lower() for word in tokenized_sentences]\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tôi',\n",
       " 'là',\n",
       " 'sinh viên',\n",
       " 'trường',\n",
       " 'đại học',\n",
       " 'bách khoa',\n",
       " '.',\n",
       " 'tôi',\n",
       " 'học',\n",
       " 'ngành',\n",
       " 'khoa học',\n",
       " 'máy tính',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "tokenize_vi('Tôi là sinh viên trường Đại học Bách Khoa. Tôi học ngành Khoa học máy tính.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer for English\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "\n",
    "# Define the tokenizer for Vietnamese\n",
    "vi_tokenizer = get_tokenizer(tokenize_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data.iloc[idx]\n",
    "        src_tokens = self.src_tokenizer(src.lower())\n",
    "        tgt_tokens = self.tgt_tokenizer(tgt.lower())\n",
    "        \n",
    "        # 1. Add '<sos>' and '<eos>' into the sentence\n",
    "        \n",
    "        src_tokens = ['<sos>'] + src_tokens + ['<eos>']\n",
    "        tgt_tokens = ['<sos>'] + tgt_tokens + ['<eos>']\n",
    "\n",
    "        #. Convert into a tensor of IDs\n",
    "        \n",
    "        src_tensor = torch.tensor([self.src_vocab[token] for token in src_tokens], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([self.tgt_vocab[token] for token in tgt_tokens], dtype=torch.long)\n",
    "        \n",
    "        return src_tensor, tgt_tensor\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, src_tokenizer, tgt_tokenizer, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.src_vocab = None\n",
    "        self.tgt_vocab = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.src_vocab is not None and self.tgt_vocab is not None:\n",
    "            return\n",
    "        \n",
    "        # Build vocabularies\n",
    "        self.src_vocab = build_vocab_from_iterator(\n",
    "            self.df['english'].apply(lambda x: self.src_tokenizer(x.lower())),\n",
    "            specials=['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        )\n",
    "    \n",
    "        # Construct the tgt_vocab\n",
    "        self.tgt_vocab = build_vocab_from_iterator(\n",
    "            self.df['vietnamese'].apply(lambda x: self.tgt_tokenizer(x.lower())),\n",
    "            specials=['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        )\n",
    "    \n",
    "        self.src_vocab.set_default_index(self.src_vocab['<unk>'])\n",
    "        \n",
    "        # Set the default index for the target vocabulary\n",
    "        \n",
    "        self.tgt_vocab.set_default_index(self.tgt_vocab['<unk>'])\n",
    "\n",
    "        # Create datasets\n",
    "        \n",
    "        self.tranlastion_dataset = TranslationDataset(\n",
    "            self.df,\n",
    "            self.src_vocab,\n",
    "            self.tgt_vocab,\n",
    "            self.src_tokenizer,\n",
    "            self.tgt_tokenizer\n",
    "        )\n",
    "        \n",
    "        train_size = int(0.8 * len(self.df))\n",
    "        val_size = len(self.df) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.tranlastion_dataset, [train_size, val_size])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        src_batch, tgt_batch = zip(*batch)\n",
    "        src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=self.src_vocab['<pad>'], batch_first=True)\n",
    "        tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=self.tgt_vocab['<pad>'], batch_first=True)\n",
    "        return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(pl.LightningModule):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_length=80):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(\n",
    "            src_vocab_size=src_vocab_size,\n",
    "            tgt_vocab_size=tgt_vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_heads=nhead,\n",
    "            num_layers=num_layers,\n",
    "            d_ff=dim_feedforward,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        return  self.transformer(src, tgt)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, tgt = batch\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        output = self(src, tgt_input)\n",
    "        \n",
    "        loss = self.loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        src, tgt = batch\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        output = self(src, tgt_input)\n",
    "        \n",
    "        loss = self.loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "        # Learning rate scheduler: Reduces LR when validation loss plateaus\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",  \n",
    "            factor=0.1, \n",
    "            patience=3, \n",
    "            verbose=False,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data module\n",
    "data_module = TranslationDataModule(df, en_tokenizer, vi_tokenizer, batch_size=32)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TranslationModel(\n",
    "    src_vocab_size=len(data_module.src_vocab),\n",
    "    tgt_vocab_size=len(data_module.tgt_vocab),\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=3,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_seq_length=80\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback: Saves the best model based on validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  \n",
    "    dirpath=\"checkpoints_machine_translation\",\n",
    "    filename=\"transformer-best-{epoch:02d}\", \\\n",
    "    save_top_k=1, \n",
    "    mode=\"min\",  \n",
    "    save_last=True, \n",
    ")\n",
    "\n",
    "# Early stopping callback: Stops training if validation loss doesn't improve\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=5, \n",
    "    verbose=True,  \n",
    "    mode=\"min\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "/Users/minhdat2004/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/minhdat2004/BKU/DL4NLP/FinalExam/checkpoints_machine_translation exists and is not empty.\n",
      "\n",
      "  | Name        | Type             | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | transformer | Transformer      | 21.6 M | train\n",
      "1 | loss_fn     | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.312    Total estimated model params size (MB)\n",
      "130       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc34e96f6e6b4a118493e16a6b835974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhdat2004/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/minhdat2004/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3cfaedf91542baaf98458236c26979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d4a3d662564a7d8dbc3bd583d72caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.630\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "# Create the model\n",
    "# Initialize the Trainer with tensorboard logger\n",
    "trainer = pl.Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    max_epochs=1,\n",
    "    accelerator=\"auto\", \n",
    "    devices=-1,  \n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=20,\n",
    "    logger=TensorBoardLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=\"transformer_translation\",\n",
    "        version=1,\n",
    "    ),\n",
    ")\n",
    "# Start the training process\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translating a list of sentences from English to Vietnamese\n",
    "def translate_sentences(model, data_module, num_sentences=10, max_len=50):\n",
    "    \"\"\"\n",
    "    Translate random sentences from English to Vietnamese in the validation set\n",
    "    Args:\n",
    "        model (nn.Module): The trained translation model.\n",
    "        data_module: The DataModule containing vocab and dataset.\n",
    "        num_sentences (int): Number of random sentences to translate.\n",
    "        max_len (int): Maximum length of translation.\n",
    "    Returns:\n",
    "        List[Tuple[str, str, str]]: List of (source, target, predicted) sentences.\n",
    "    \"\"\"\n",
    "    src_vocab = data_module.src_vocab\n",
    "    tgt_vocab = data_module.tgt_vocab\n",
    "    src_tokenizer = data_module.src_tokenizer\n",
    "    tgt_tokenizer = data_module.tgt_tokenizer\n",
    "\n",
    "    PAD_IDX = tgt_vocab['<pad>']\n",
    "    SOS_IDX = tgt_vocab['<sos>']\n",
    "    EOS_IDX = tgt_vocab['<eos>']\n",
    "    SRC_PAD_IDX = src_vocab['<pad>']\n",
    "    SRC_SOS_IDX = src_vocab['<sos>']\n",
    "    SRC_EOS_IDX = src_vocab['<eos>']\n",
    "\n",
    "    # Select random sentences from validation set\n",
    "    random_indices = torch.randint(0, len(data_module.val_dataset), (num_sentences,))\n",
    "    random_sentences = [data_module.val_dataset[i] for i in random_indices]\n",
    "\n",
    "    translations = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src_tensor, tgt_tensor in random_sentences:\n",
    "            # Prepare source sentence\n",
    "            src_tokens = [src_vocab.lookup_token(token.item()) for token in src_tensor]\n",
    "            # Remove special tokens <sos>, <eos>, <pad>\n",
    "            src_tokens = [token for token in src_tokens if token not in ['<sos>', '<eos>', '<pad>']]\n",
    "            src_sentence = ' '.join(src_tokens)\n",
    "\n",
    "            # Prepare input for model\n",
    "            src_tensor = src_tensor.unsqueeze(0)  # [1, src_len]\n",
    "            tgt_input = torch.tensor([[SOS_IDX]], device=src_tensor.device)  # [1, 1] with <sos>\n",
    "\n",
    "            # Autoregressive decoding\n",
    "            for _ in range(max_len):\n",
    "                output = model(src_tensor, tgt_input)\n",
    "                next_token = output[:, -1, :].argmax(dim=-1)  # take the last token prediction\n",
    "                tgt_input = torch.cat([tgt_input, next_token.unsqueeze(0)], dim=1)\n",
    "                if next_token.item() == EOS_IDX:\n",
    "                    break\n",
    "\n",
    "            # Decode target ground truth\n",
    "            tgt_tokens = [token.item() for token in tgt_tensor]\n",
    "            tgt_tokens = [token for token in tgt_tokens if token not in {PAD_IDX, SOS_IDX, EOS_IDX}]\n",
    "            tgt_sentence = ' '.join([tgt_vocab.lookup_token(token) for token in tgt_tokens])\n",
    "\n",
    "            # Decode predicted translation\n",
    "            pred_tokens = [token.item() for token in tgt_input[0]]\n",
    "            pred_tokens = [token for token in pred_tokens if token not in {PAD_IDX, SOS_IDX, EOS_IDX}]\n",
    "            pred_sentence = ' '.join([tgt_vocab.lookup_token(token) for token in pred_tokens])\n",
    "\n",
    "            translations.append((src_sentence, tgt_sentence, pred_sentence))\n",
    "\n",
    "    return translations\n",
    "\n",
    "# Translate one sentence \n",
    "def translate_one_sentence(model, data_module, sentence, max_len=50):\n",
    "    \"\"\"\n",
    "    Translate a single sentence from English to Vietnamese\n",
    "    Args:\n",
    "        model (nn.Module): The trained translation model.\n",
    "        data_module: The DataModule containing vocab and dataset.\n",
    "        sentence (str): The English sentence to translate.\n",
    "        max_len (int): Maximum length of translation.\n",
    "    Returns:\n",
    "        Tuple[str, str, str]: (source, target, predicted) sentences.\n",
    "    \"\"\"\n",
    "    src_vocab = data_module.src_vocab\n",
    "    tgt_vocab = data_module.tgt_vocab\n",
    "    src_tokenizer = data_module.src_tokenizer\n",
    "    tgt_tokenizer = data_module.tgt_tokenizer\n",
    "\n",
    "    PAD_IDX = tgt_vocab['<pad>']\n",
    "    SOS_IDX = tgt_vocab['<sos>']\n",
    "    EOS_IDX = tgt_vocab['<eos>']\n",
    "    \n",
    "    # Prepare source sentence\n",
    "    src_tokens = src_tokenizer(sentence.lower())\n",
    "    src_tokens = ['<sos>'] + src_tokens + ['<eos>']\n",
    "    src_tensor = torch.tensor([src_vocab[token] for token in src_tokens], dtype=torch.long).unsqueeze(0)  # [1, src_len]\n",
    "    \n",
    "    # Prepare input for model\n",
    "    tgt_input = torch.tensor([[SOS_IDX]], device=src_tensor.device)  # [1, 1] with <sos>\n",
    "\n",
    "    # Autoregressive decoding\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(src_tensor, tgt_input)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1)  # take the last token prediction\n",
    "            tgt_input = torch.cat([tgt_input, next_token.unsqueeze(0)], dim=1)\n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "\n",
    "    # Decode predicted translation\n",
    "    pred_tokens = [token.item() for token in tgt_input[0]]\n",
    "    pred_tokens = [token for token in pred_tokens if token not in {PAD_IDX, SOS_IDX, EOS_IDX}]\n",
    "    pred_sentence = ' '.join([tgt_vocab.lookup_token(token) for token in pred_tokens])\n",
    "\n",
    "    return sentence, pred_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Translations ---\n",
      "Source:    what you need to do next is fill out this application form .\n",
      "Target:    những gì bạn cần làm tiếp theo là điền vào mẫu đơn này .\n",
      "Predicted: những gì bạn cần phải làm tiếp theo là điền vào mẫu đơn này .\n",
      "--------------------------------------------------\n",
      "Source:    it will cost 500 dollars to fly to paris .\n",
      "Target:    nó sẽ có giá 500 đô la để bay đến paris .\n",
      "Predicted: nó sẽ tốn 500 đô la để bay đến paris .\n",
      "--------------------------------------------------\n",
      "Source:    we 're not the only ones here from boston\n",
      "Target:    chúng tôi không phải là những người duy nhất ở đây từ boston\n",
      "Predicted: chúng tôi không phải là người duy nhất ở đây từ boston\n",
      "--------------------------------------------------\n",
      "Source:    she lives far from there .\n",
      "Target:    cô ấy sống xa đó\n",
      "Predicted: cô ấy sống xa đây .\n",
      "--------------------------------------------------\n",
      "Source:    you 're the most beautiful girl i 've ever seen\n",
      "Target:    bạn là cô gái đẹp nhất tôi từng thấy\n",
      "Predicted: bạn là cô gái xinh đẹp nhất tôi từng thấy\n",
      "--------------------------------------------------\n",
      "Source:    nobody invited me to the party\n",
      "Target:    không ai mời tôi đến bữa tiệc\n",
      "Predicted: không ai mời tôi đến bữa tiệc\n",
      "--------------------------------------------------\n",
      "Source:    tom was injured\n",
      "Target:    tom bị thương\n",
      "Predicted: tom bị thương\n",
      "--------------------------------------------------\n",
      "Source:    as a new father , i gave my first child plenty of books\n",
      "Target:    là một người cha mới , tôi đã cho đứa con đầu lòng của mình rất nhiều sách\n",
      "Predicted: như một người cha mới , tôi đã cho con mình rất nhiều sách\n",
      "--------------------------------------------------\n",
      "Source:    the stock market is in a prolonged slump .\n",
      "Target:    thị trường chứng khoán rơi vào suy thoái kéo dài .\n",
      "Predicted: thị trường nằm trong một chiếc khăn\n",
      "--------------------------------------------------\n",
      "Source:    let 's stay home and watch tv .\n",
      "Target:    hãy ở nhà và xem tv .\n",
      "Predicted: hãy ở nhà và xem tivi\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "translations = translate_sentences(model, data_module)\n",
    "\n",
    "# --- Print the results ---\n",
    "print(\"\\n--- Translations ---\")\n",
    "for src, tgt, pred in translations:\n",
    "    print(f\"Source:    {src[:-2]}\")\n",
    "    print(f\"Target:    {tgt}\")\n",
    "    print(f\"Predicted: {pred}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU score for 10 translations (torchmetrics): 0.6552\n"
     ]
    }
   ],
   "source": [
    "# Calculate the BLEU score\n",
    "\n",
    "bleu = BLEUScore()\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "for src, tgt, pred in translations:\n",
    "    references.append([tgt])   # Ground truth (as list of strings)\n",
    "    hypotheses.append(pred)    # Model output (as string)\n",
    "    \n",
    "\n",
    "bleu_score = bleu(hypotheses, references)\n",
    "\n",
    "print(f\"\\nBLEU score for 10 translations (torchmetrics): {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    Hello, how are you?\n",
      "Predicted: xin chào , bạn thế nào ?\n",
      "Source:    The cat is sleeping on mat\n",
      "Predicted: con mèo đang ngủ trên tấm thảm\n",
      "Source:    you are so beautiful\n",
      "Predicted: bạn thật đẹp\n",
      "Source:    I am a student at Hanoi University of Science and Technology\n",
      "Predicted: tôi là học sinh tại trường đại học và công nghệ\n",
      "Source:    I love you\n",
      "Predicted: tôi yêu bạn\n"
     ]
    }
   ],
   "source": [
    "# Test the translation of a single sentence\n",
    "sentence = \"Hello, how are you?\"\n",
    "src, pred = translate_one_sentence(model, data_module, sentence)\n",
    "print(f\"Source:    {src}\")\n",
    "print(f\"Predicted: {pred}\")\n",
    "# Test the translation of a single sentence\n",
    "sentence = \"The cat is sleeping on mat\"\n",
    "src, pred = translate_one_sentence(model, data_module, sentence)\n",
    "print(f\"Source:    {src}\")\n",
    "print(f\"Predicted: {pred}\")\n",
    "sentence = \"you are so beautiful\"\n",
    "src, pred = translate_one_sentence(model, data_module, sentence)\n",
    "print(f\"Source:    {src}\")\n",
    "print(f\"Predicted: {pred}\")\n",
    "sentence = \"I am a student at Hanoi University of Science and Technology\"\n",
    "src, pred = translate_one_sentence(model, data_module, sentence)\n",
    "print(f\"Source:    {src}\")\n",
    "print(f\"Predicted: {pred}\")\n",
    "sentence = \"I love you\"\n",
    "src, pred = translate_one_sentence(model, data_module, sentence)\n",
    "print(f\"Source:    {src}\")\n",
    "print(f\"Predicted: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
